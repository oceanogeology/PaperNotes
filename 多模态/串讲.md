# 多模态串讲

### 网络架构

1. 文本编码小于视觉编码器
2. 融合后网络也需要比较大

### Loss

1. MLM：mask language modeling
2. ITM：Image-Text Macthing：二分类任务
3. ITC：Image-Text Contrastive loss：对比学习任务



### 一. ALBEF: Align Before Fusing...（单卡训练4天）

#### Contribution

1.   Align before fuse: 通过特征维度上进行contrastive loss达到
2. 解决noisy web data：通过制造momentum distillation

#### Network

1. 在融合特征之前做了ITCloss，可以提前Align
2. 在ITM二分类时候，通过ITC阶段的相似度取得hard negative作为负样本
3. MLM，把文本中某个单词mask掉，然后在结果预测输出。**注意：此时前向的时候文本编码器输入的是mask掉的 T', 而ITC和ITM中计算loss的时候是完整的T，故在大多数多模态架构里面，需要前向2次，或者一次前向2个文本的特征，因此会比较慢。**

<img src="..\images\image-20230130135548467.png" alt="image-20230130135548467" style="zoom:80%;" />

#### Momentum distillation

为了解决数据中存在的噪声，某些GT会存在问题，这时候可以使用自训练模型（动量EMA）得到的***pseudo targets***来替代GT

1. ITCloss: GT和pseudo model 分配权重来计算loss，由于pseudo targets是softmax label，故计算loss时候可以用KL散度计算。

   ![image-20230130144810904](..\images\image-20230130144810904.png)

   <img src="..\images\image-20230130150112396.png" alt="image-20230130150112396" style="zoom:80%;" />

2. MTMloss: 同上

   ![image-20230130145421806](..\images\image-20230130145421806.png)

   <img src="..\images\image-20230130150210503.png" alt="image-20230130150210503" style="zoom:80%;" />

3. 无ITMloss，二分类任务必须知道 图文是不是一对，而且在也进行了hard negative的操作，和动量model的可替代性有冲突。

#### Downstream V+L Tasks（视觉理解）

1. Image-Text Retrieval（图文检索）：2种：文本检索图像，图像检索文本。
2. Visual Entailment（视觉蕴含）：3分类，图文是不是存在关系
3. Visual Question Answering（视觉问答）：给定一个问题和图片，看能否回答问题提供答案。2种：闭集VQA，分类问题，答案就那么一个set，从里面选；开集VQA，文本生成问题。
4. Visual Reasoning（视觉推理）：一个文本能不能同时描述一对图片。

#### Downstream V+L Tasks（视觉生成）：需要添加transformer的Decorder。Language modeling: 给定一些词，预测后续的词



### 二. VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts （训练很慢，在4million数据量上，64张卡需要2天）

#### Contribution

1. 分阶段训练：先进行视觉训练视觉，再进行文本训练文本，最后融合再训练。

   <img src="..\images\image-20230130155028014.png" alt="image-20230130155028014" style="zoom:80%;" />

2. 不同模态都有各自的experts，具有灵活性，这里的experts是transformer block里面的FFN的不同，在计算ITCloss的时候，视觉编码是V-FFN，文本编码是L-FFN；在计算ITMloss和MLMloss的时候，融合层是用的VL-FFN。

<img src="..\images\image-20230130154452643.png" alt="image-20230130154452643" style="zoom:80%;" />

#### Results: (在VQA和VR上都比ALBEF高3个点)

<img src="..\images\image-20230130155429786.png" alt="image-20230130155429786" style="zoom:80%;" />

#### Perspective（作者团队的后期工作确实都把future work都实现了，YYDS!!!）

<img src="..\images\image-20230130155606200.png" alt="image-20230130155606200" style="zoom:80%;" />



### 三. BLIP：Bootstrapping Language-Image Pre-Training for Unified Vision-Language Understanding and Generation

#### Contribution

1. 吸收了ALBEF和VLMO的思想，ALBEF：只看ITC和ITM部分就是一个ALBEF的架构，VLMO：SA（self-attention）和FFN（feed forword network）是参数共享的，CA是新加的（ITM部分）。
2. 生成部分使用Causual self-attention 层，FF和CA是共享的（LM部分）。

<img src="..\images\image-20230130163643833.png" alt="image-20230130163643833" style="zoom:80%;" />

3. capfilter：通过Encoder的相似度高的训练一个Filter，用来清洗原始的文本Tw → Tw‘ , 然后根据Decoder对所有Iw生成文本Ts。

<img src="..\images\image-20230130164735730.png" alt="image-20230130164735730" style="zoom:80%;" />

​     capfiler的效果：Filter做选择（选的是绿色），Captioner是生成文本Ts

<img src="..\images\image-20230131090530936.png" alt="image-20230131090530936" style="zoom:80%;" />

#### Perspective：有很多跟进BLIP的工作，比如laion.ai利用BLIP的captioner优化了很多数据集



### 四. CoCa: Contrastive Captioners are Image-Text Foundation Models

#### Contribution:

1. network和albef很像，只是文本端使用了Decoder。
2. 为了避免forward多次，减少训练时长，文本输入时采用Causual SA，也就是mask掉部分文本的方式直接输入，计算ITCloss和CaptionLoss
3. 使用了超大规模的数据和参数进行训练（实现不难，复现太难）

<img src="..\images\image-20230131092822480.png" alt="image-20230131092822480" style="zoom:80%;" />

### Result:

<img src="..\images\image-20230131093503945.png" alt="image-20230131093503945" style="zoom:80%;" />



### 五. BeiTv3: Image as a Foreign Language: BeiT Pretraining for All Vision and Vision-Language Tasks

#### Contribution

1. 框架大一统：将图片也看成是一种语言Imagish，只用了一个模型结构（Transformer），一个目标函数（Mask Modeling loss）

#### Method

1. 和VLMO很像，可根据具体下游任务的形式进行使用不同模块

<img src="..\images\image-20230131095317433.png" alt="image-20230131095317433" style="zoom:80%;" />

​     推理：

<img src="..\images\image-20230131095710034.png" alt="image-20230131095710034" style="zoom:80%;" />

#### Results

<img src="..\images\image-20230131094117334.png" alt="image-20230131094117334" style="zoom:80%;" />



### 总结：

<img src="..\images\image-20230131100526242.png" alt="image-20230131100526242" style="zoom:80%;" />



