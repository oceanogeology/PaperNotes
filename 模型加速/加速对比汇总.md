测试条件：  seq_len = 128      batch = 4    loop = 100

|  模型  | 加速比 | torch | SPeed |       Frame       | 量化 |   machine   |                 tag                  |
| :----: | :----: | ----- | :---: | :---------------: | :--: | :---------: | :----------------------------------: |
|  bert  |  2.9   | 1.26s | 0.43s |     Fuxi-EET      | fp16 | 2080ti(122) | 12 layer; hidden_size:768; 12 heads; |
|  bert  |  1.2   | 1.3s  | 1.08s |     Fuxi-EET      | fp32 | 2080ti(122) |                                      |
| albert |  2.9   | 1.32  | 0.46  |     Fuxi-EET      | fp16 | 2080ti(122) | 12 layer; hidden_size:768; 12 heads; |
| albert |  1.2   | 1.3   | 1.19  |     Fuxi-EET      | fp32 | 2080ti(122) |                                      |
| albert |  2.9   | 0.46  | 0.12  |     Fuxi-EET      | fp16 | 2080ti(122) |       ==线上标点模型：4layer==       |
|  bert  |  1.3   | 1.75s | 1.32s | Turbo-transformer | fp32 | 1080ti(53)  |                                      |
|  bert  |  1.6   | 1.58  |   1   | Turbo-transformer | fp32 | 2080ti(122) |                                      |
| albert |  1.4   | 1.75s | 1.25s | Turbo-transformer | fp32 | 1080ti(53)  |                                      |
| albert |  1.8   | 1.95  | 1.07  | Turbo-transformer | fp32 | 2080ti(122) |                                      |







### fuxi-eet框架的加速效果，结论如下：

* 框架提供的接口有限，主要针对bert和gpt2进行了封装，如果模型跟这两个模型相差较大，修改起来较为困难。
* 加速比，还是不错的。
* 使用的时候，如果接口没有实现，可以直接使用pytorch的模型接口进行计算。
* 比如对于finetune的模型，就可以直接在后面接fc进行计算。



### turbo transformer:

- 其接口相比fuxi-eet要多一些，实现的模型比较多，修改起来也还ok，难度跟eet差不多。
- 但是其对fp16的支持不是很好，好像没有针对这块来做，这点没有fuxi-eet人性化。
- 但是针对fp32的优化速度是比fuxi-eet要快一些的，但是eet的fp16确实要快一些。
- https://github.com/Tencent/TurboTransformers

### faster transformer

- 简单看了介绍，暂时不尝试了，入门难度会比前面这两个大。