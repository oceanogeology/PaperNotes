## 什么时候需要Feature Scaling，什么时候不需要

**归一化/标准化的目的是为了获得某种“无关性”——偏置无关、尺度无关、长度无关……当归一化/标准化方法背后的物理意义和几何含义与当前问题的需要相契合时，其对解决该问题就有正向作用，反之，就会起反作用。所以，“何时选择何种方法”取决于待解决的问题，即problem-dependent。**

需要：涉及或隐含**距离计算**的算法，比如K-means、KNN、PCA、SVM等；损失函数中含有**正则项**时；**梯度下降算法，需要feature scaling**

不需要：与距离计算无关的概率模型，不需要feature scaling，比如Naive Bayes；与距离计算无关的基于树的模型，不需要feature scaling，比如决策树、随机森林等，树中节点的选择只关注当前特征在哪里切分对分类更好，即只在意特征内部的相对大小，而与特征间的相对大小无关。

注：normalize其并不会改变各个维度特征的相关性。通过PCA方法可以移除线性相关性；



