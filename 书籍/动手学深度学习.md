# GRU

torch.nn.GRU()

门控循环单元

这个其实就是在RNN的基础上增加了门控的参数控制，

标准的RNN的公式： $H_t = tanh(X_tW_{xh}+H_{t-1}W_{hh}+b_n)$

GRU就是首先在$H_{t-1}$上加入了系数控制，意思为**重置门**，如果系数为0，就把历史信息重置了；然后得到$H_t$之后，将本次的$H_t$和历史的$H_t$​进行系数的融合，意思为**更新门**，更新H的信息。

![image-20210810153546409](..\images\image-20210810153546409.png)

# LSTM

torch.nn.LSTM()

lstm就是在设置了**输入门，输出门，和遗忘门**；利用遗忘门和输入门，来控制H和输入的比例，然后利用输出门来调节H的输出比例，仅此而已。重点在于下面最后面的两行，三个门的融合计算。

![image-20210811095617925](..\images\image-20210811095617925.png)

## 双向循环



* 前向和候选的H合并， size为：[2 *  batch * num_hidden];  bidirectional参数可以控制； 
* 但是由于在预测的时候很少会有未来信息，所以双向循环使用场景有限，效果并不是很好。
* 双向可以做encoder，不可以做decoder，经常用在encoder里面。

### nn.Embding

* 我们使用了 *嵌入层*（embedding layer）来获得输入序列中每个词元的特征向量。嵌入层的权重是一个矩阵，其行数等于输入词表的大小（`vocab_size`），其列数等于特征向量的维度（`embed_size`），==但是一般情况，embed_size = hidden size==

* valid_len[:, None]  **torch，Tensor中增加一个维度，比较简单的写法**

* **python 如何调用父类：**  unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(

  ​      pred.permute(0, 2, 1), label)

### endocer decoder

* 需要做init_state的状态，将encoder的输出初始化给decoder的state
* seq2seq : encoder最后的state会与decoder的输入拼接起来
* seq2seq：训练的时候，decoder输入的是label 不会出现长时间错了的情况，推理的时候会依赖上一个时刻的输出。
* 

### Transformer

* state包含N个layers,训练时候初始化为None
* multihead attention中，Q,K,V的输出维度是一样的，但是输入维度可以不一样，一般K V是一样的，最终的维度还是由Q来决定的。
* 在infer的时候，输入X会与state，concat在一起，当做K 和 V 来进行计算，state会随着解码越来越长。
* encoder的输出是作用在decoder的每个layer的第二个multi-head中的，而且每个的输入都是一样的都是encoder的输出，encoder输出作为的是 K 和  V
* 在transformer中，没有了像rnn中，将step移到batch前面的操作，而是直接利用nn.linear进行计算的。所以是全连接。



