# 大模型时代下的科研

<img src="../images/image-20230419112417449.png" alt="image-20230419112417449" style="zoom:50%;" />

### Efficient(Parameter-Efficient Fine-Tune)

综述：Towards a unified view of parameter-efficient transfer learning

#### 1. Adapter

<img src="../images/image-20230419113619181.png" alt="image-20230419113619181" style="zoom:70%;" />

- Transformer中新加Adapter层，只训练这个层，有时训练成本可以降为原始大模型的万分之一。

#### 2. CoOp(Prompt tuning)

<img src="../images/image-20230419134355287.png" alt="image-20230419134355287" style="zoom:80%;" />

#### 3. VPT(Visual Prompt tuning)

<img src="../images/image-20230419133834112.png" alt="image-20230419133834112" style="zoom:80%;" />

#### 4. huggingface PEFT blog

GPU内存不高，如何使用大模型  https://github.com/huggingface/peft

#### 5. AIM

https://adapt-image-models.github.io/

视频理解的PEFT,参照Adapter。c：空间维度加adapter，d：时序、空间维度加adapter，e：

<img src="../images/image-20230419140714598.png" alt="image-20230419140714598" style="zoom:80%;" />

![image-20230419141006516](../images/image-20230419141006516.png)

### Existing Model

#### 1. Object-Centric Learning

<img src="../images/image-20230419145759401.png" alt="image-20230419145759401" style="zoom:80%;" />

<img src="../images/image-20230419145914845.png" alt="image-20230419145914845" style="zoom:80%;" />

### Plug and Play

#### 1. MixGen: data augmentation

<img src="../images/image-20230419152015935.png" alt="image-20230419152015935" style="zoom:80%;" />

### Evaluation\new dataset\survey

#### 1. 目标检测集大成者的数据集

![image-20230419152550688](../images/image-20230419152550688.png)